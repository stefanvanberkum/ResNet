{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87Ncez2_RHbE",
    "outputId": "86b4c7a8-b735-418e-a4fa-e4ed6564b553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.7/site-packages (0.19.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons) (23.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons) (2.13.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gA1LasNvUQ36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:32:30.144413: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-08 20:32:32.732590: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-08 20:32:32.732714: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-08 20:32:32.732727: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-05-08 20:32:37.579723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 20:32:37.592141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 20:32:37.593839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 20:32:37.595921: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-08 20:32:37.596422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 20:32:37.598063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 20:32:37.599667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 20:32:38.304549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 20:32:38.306259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 20:32:38.307732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-08 20:32:38.309133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot\n",
    "from tensorflow import pad\n",
    "from tensorflow.keras.datasets import cifar10, cifar100\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D, Dense, BatchNormalization, Normalization, Add\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow_addons.layers import StochasticDepth\n",
    "from tensorflow.image import stateless_random_flip_left_right, stateless_random_crop\n",
    "from tensorflow.random.experimental import stateless_split\n",
    "from tensorflow.data import AUTOTUNE\n",
    "\n",
    "# Load CIFAR-10 data.\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Initialize normalization layer.\n",
    "norm_layer = Normalization(axis=(1, 2, 3), mean=X_train.mean(axis=0), variance=1)\n",
    "\n",
    "# Convert from integers to floats.\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Apply normalization.\n",
    "X_train = norm_layer(X_train)\n",
    "X_test = norm_layer(X_test)\n",
    "\n",
    "# Data augmentation.\n",
    "# Pad X_train.\n",
    "X_train = pad(X_train, [[0, 0], [4, 4], [4, 4], [0, 0]])\n",
    "\n",
    "# Augmentation function.\n",
    "def augment(image_label, seed):\n",
    "    image, label = image_label\n",
    "    \n",
    "    # Make a new seed.\n",
    "    new_seed = stateless_split(seed, num=1)[0, :]\n",
    "    \n",
    "    # Randomly flip and crop.\n",
    "    image = stateless_random_flip_left_right(image, seed=seed)\n",
    "    image = stateless_random_crop(image, size=[32, 32, 3], seed=new_seed)\n",
    "    return image, label\n",
    "\n",
    "rng = tf.random.Generator.from_seed(123, alg='philox')\n",
    "# Wrapper function.\n",
    "def f(x, y):\n",
    "    seed = rng.make_seeds(2)[0]\n",
    "    image, label = augment((x, y), seed)\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_ds = train_ds.shuffle(1280, reshuffle_each_iteration=True).map(f, num_parallel_calls=AUTOTUNE).batch(128).prefetch(AUTOTUNE)\n",
    "\n",
    "path = 'cifar10/'\n",
    "with open(path + 'results.csv', 'w') as results:\n",
    "    writer = csv.writer(results)\n",
    "    writer.writerow(['n', 'p_L', 'acc', 'time (s)', 'time (min)'])\n",
    "\n",
    "def save_result(n, p_L, history, t, path):\n",
    "    \"\"\"Evaluate the model and store the results.\n",
    "\n",
    "    :param n: Network size parameter.\n",
    "    :param p_L: Final survival probability for linear decay.\n",
    "    :param history: Model training history object.\n",
    "    :param t: Training time.\n",
    "    :param path: Path to results.\n",
    "    \"\"\"\n",
    "    \n",
    "    _, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test accuracy: %.3f' % (acc * 100.0))\n",
    "    with open(path + 'results.csv', 'a') as results:\n",
    "      writer = csv.writer(results)\n",
    "      writer.writerow([n, p_L, round(100 * acc, 2), round(t), round(t / 60)])\n",
    "\n",
    "    prefix = f\"{path}raw/{n}_{int(100 * p_L)}_\"\n",
    "\n",
    "    train_loss_path = prefix + 'train_loss.npy'\n",
    "    test_loss_path = prefix + 'test_loss.npy'\n",
    "    train_acc_path = prefix + 'train_acc.npy'\n",
    "    test_acc_path = prefix + 'test_acc.npy'\n",
    "\n",
    "    with open(train_loss_path, 'wb') as f:\n",
    "        np.save(f, history.history['loss'])\n",
    "\n",
    "    with open(test_loss_path, 'wb') as f:\n",
    "        np.save(f, history.history['val_loss'])\n",
    "\n",
    "    with open(train_acc_path, 'wb') as f:\n",
    "        np.save(f, history.history['accuracy'])\n",
    "\n",
    "    with open(test_acc_path, 'wb') as f:\n",
    "        np.save(f, history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0sSnQ2L7W26J"
   },
   "outputs": [],
   "source": [
    "def ResNet(n, n_out, p_L, weight_decay=1e-4, bottleneck=False):\n",
    "  \"\"\"ResNet for CIFAR-10 as described by He et al. (2015).\n",
    "\n",
    "  This network has 2n + 2 layers.\n",
    "  \n",
    "  :param n: Network size parameter.\n",
    "  :param n_out: Output size.\n",
    "  :param p_L: Final survival probability for linear decay.\n",
    "  :param weight_decay: Weight decay parameter.\n",
    "  :param bottleneck: True if bottleneck blocks should be used.\n",
    "  \"\"\"\n",
    "\n",
    "  stages = [16, 32, 64]\n",
    "  inputs = Input(shape=(32, 32, 3))\n",
    "\n",
    "  n_blocks = 3 * n\n",
    "  block_count = 1\n",
    "  \n",
    "  # First convolution.\n",
    "  x = Conv2D(16, 3, 1, padding='same', use_bias=False, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(inputs)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = relu(x)\n",
    "    \n",
    "  # ResNet blocks.\n",
    "  for i in range(len(stages)):\n",
    "    n_filters = stages[i]\n",
    "    for j in range(n):\n",
    "      # Compute survival probability using linear decay.\n",
    "      p_s = 1 - block_count / n_blocks * (1 - p_L)\n",
    "        \n",
    "      # Add ResNet block.\n",
    "      if bottleneck:\n",
    "        # Use bottleneck blocks.\n",
    "        if i == 0 and j == 0:\n",
    "          x = bottleneck_block(x, n_filters, p_s, weight_decay, first=True)\n",
    "        elif j == 0:\n",
    "          x = bottleneck_block(x, n_filters, p_s, weight_decay, downsample=True)\n",
    "        else:\n",
    "          x = bottleneck_block(x, n_filters, p_s, weight_decay)\n",
    "      else:\n",
    "        # Use classic ResNet blocks.\n",
    "        if i > 0 and j == 0:\n",
    "          x = block(x, n_filters, p_s, weight_decay, downsample=True)\n",
    "        else:\n",
    "          x = block(x, n_filters, p_s, weight_decay)\n",
    "      block_count += 1\n",
    "    \n",
    "  # Pooling and dense output layer with softmax activation.\n",
    "  x = GlobalAveragePooling2D()(x)\n",
    "  outputs = Dense(n_out, activation='softmax', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x)\n",
    "\n",
    "  model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "  # Compile model.\n",
    "  opt = SGD(learning_rate=0.1, momentum=0.9)\n",
    "  model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "\n",
    "def block(x, n_filters, p_s, weight_decay, downsample=False, use_conv=False):\n",
    "  \"\"\"Classic ResNet block.\n",
    "  \n",
    "  :param x: Input.\n",
    "  :param n_filters: Number of filters.\n",
    "  :param p_s: Survival probability.\n",
    "  :param weight_decay: Weight decay parameter.\n",
    "  :param downsample: True if the layer should downsample.\n",
    "  :param use_conv: True if a convolution operation should be used to match residual dimensions on downsampling.\n",
    "  \"\"\"\n",
    "  \n",
    "  if downsample:\n",
    "    start_stride = 2\n",
    "  else:\n",
    "    start_stride = 1\n",
    "    \n",
    "  x_skip = x\n",
    "    \n",
    "  x = Conv2D(n_filters, 3, start_stride, padding='same', use_bias=False, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = relu(x)\n",
    "  x = Conv2D(n_filters, 3, 1, padding='same', use_bias=False, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  if downsample:\n",
    "    if use_conv:\n",
    "      x_skip = Conv2D(n_filters, 1, 2, use_bias=False, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x_skip)\n",
    "      x_skip = BatchNormalization()(x_skip)\n",
    "    else:\n",
    "      x_skip = x_skip[:, ::2, ::2, :]\n",
    "      missing = n_filters - x_skip.shape[3]\n",
    "      x_skip = pad(x_skip, [[0, 0], [0, 0], [0, 0], [missing // 2, -(missing // -2)]])\n",
    "  \n",
    "  if p_s == 1:\n",
    "    x = Add()([x_skip, x])\n",
    "  else:\n",
    "    x = StochasticDepth(p_s)([x_skip, x])\n",
    "  x = relu(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "\n",
    "def bottleneck_block(x, n_filters, p_s, weight_decay, first=False, downsample=False, use_conv=False):\n",
    "  \"\"\"Bottleneck ResNet block.\n",
    "  \n",
    "  :param x: Input.\n",
    "  :param n_filters: Number of filters.\n",
    "  :param p_s: Survival probability.\n",
    "  :param weight_decay: Weight decay parameter.\n",
    "  :param first: True if this is the first block.\n",
    "  :param downsample: True if the layer should downsample.\n",
    "  :param use_conv: True if a convolution operation should be used to match residual dimensions on downsampling.\n",
    "  \"\"\"\n",
    "  \n",
    "  if downsample:\n",
    "    start_stride = 2\n",
    "  else:\n",
    "    start_stride = 1\n",
    "    \n",
    "  x_skip = x\n",
    "\n",
    "  x = Conv2D(n_filters, 1, start_stride, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = relu(x)\n",
    "  x = Conv2D(n_filters, 3, 1, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = relu(x)\n",
    "  x = Conv2D(4 * n_filters, 1, 1, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  if downsample:\n",
    "    if use_conv:\n",
    "      x_skip = Conv2D(4 * n_filters, 1, 2, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x_skip)\n",
    "      x_skip = BatchNormalization()(x_skip)\n",
    "    else:\n",
    "      x_skip = x_skip[:, ::2, ::2, :]\n",
    "      missing = n_filters - x_skip.shape[3]\n",
    "      x_skip = pad(x_skip, [[0, 0], [0, 0], [0, 0], [missing // 2, -(missing // -2)]])\n",
    "  elif first:\n",
    "    if use_conv:\n",
    "      x_skip = Conv2D(4 * n_filters, 1, 1, kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(x_skip)\n",
    "      x_skip = BatchNormalization()(x_skip)\n",
    "    else:\n",
    "      missing = n_filters - x_skip.shape[3]\n",
    "      x_skip = pad(x_skip, [[0, 0], [0, 0], [0, 0], [missing // 2, -(missing // -2)]])\n",
    "  \n",
    "  if p_s == 1:\n",
    "    x = Add()([x_skip, x])\n",
    "  else:\n",
    "    x = StochasticDepth(p_s)([x_skip, x])\n",
    "  x = relu(x)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch == 82:\n",
    "      return lr / 10\n",
    "    else:\n",
    "      return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:32:43.751415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n",
      "2023-05-08 20:32:45.221649: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f2d2400ae60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-08 20:32:45.221693: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-05-08 20:32:45.294051: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "# Dummy model to get accurate time measurements.\n",
    "n = 2\n",
    "n_out = 10\n",
    "p_L = 0.9\n",
    "model = ResNet(n, n_out, p_L)\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "history = model.fit(train_ds, epochs=1, validation_data=(X_test, y_test), callbacks=[callback], verbose=0)\n",
    "tf.keras.backend.clear_session()\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hsQPS9HeW0HK",
    "outputId": "540105f8-5acd-47ae-ffcd-3f284d1782d2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ResNet with n = 1 and p_L = 1...\n",
      "Epoch 1/100\n",
      "391/391 [==============================] - 13s 30ms/step - loss: 1.6063 - accuracy: 0.4265 - val_loss: 1.5713 - val_accuracy: 0.4709 - lr: 0.1000\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 1.1997 - accuracy: 0.5964 - val_loss: 1.2596 - val_accuracy: 0.5778 - lr: 0.1000\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 1.0485 - accuracy: 0.6565 - val_loss: 1.1440 - val_accuracy: 0.6195 - lr: 0.1000\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.9550 - accuracy: 0.6952 - val_loss: 1.2534 - val_accuracy: 0.6140 - lr: 0.1000\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.8958 - accuracy: 0.7197 - val_loss: 1.0449 - val_accuracy: 0.6644 - lr: 0.1000\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.8584 - accuracy: 0.7393 - val_loss: 1.1978 - val_accuracy: 0.6468 - lr: 0.1000\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.8186 - accuracy: 0.7526 - val_loss: 1.0674 - val_accuracy: 0.6868 - lr: 0.1000\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7975 - accuracy: 0.7640 - val_loss: 1.2753 - val_accuracy: 0.6449 - lr: 0.1000\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.7818 - accuracy: 0.7704 - val_loss: 1.5949 - val_accuracy: 0.6032 - lr: 0.1000\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.7627 - accuracy: 0.7796 - val_loss: 0.8483 - val_accuracy: 0.7566 - lr: 0.1000\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7507 - accuracy: 0.7843 - val_loss: 0.8859 - val_accuracy: 0.7354 - lr: 0.1000\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7385 - accuracy: 0.7904 - val_loss: 0.9777 - val_accuracy: 0.7157 - lr: 0.1000\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7365 - accuracy: 0.7908 - val_loss: 1.1253 - val_accuracy: 0.6876 - lr: 0.1000\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7213 - accuracy: 0.7973 - val_loss: 1.0752 - val_accuracy: 0.7114 - lr: 0.1000\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 11s 27ms/step - loss: 0.7157 - accuracy: 0.8026 - val_loss: 1.6248 - val_accuracy: 0.5632 - lr: 0.1000\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7068 - accuracy: 0.8031 - val_loss: 0.9844 - val_accuracy: 0.7149 - lr: 0.1000\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7022 - accuracy: 0.8062 - val_loss: 0.9762 - val_accuracy: 0.7242 - lr: 0.1000\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6923 - accuracy: 0.8109 - val_loss: 1.2706 - val_accuracy: 0.6447 - lr: 0.1000\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6889 - accuracy: 0.8124 - val_loss: 1.1613 - val_accuracy: 0.6801 - lr: 0.1000\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6874 - accuracy: 0.8122 - val_loss: 0.8825 - val_accuracy: 0.7559 - lr: 0.1000\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6826 - accuracy: 0.8151 - val_loss: 1.1331 - val_accuracy: 0.6864 - lr: 0.1000\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6814 - accuracy: 0.8164 - val_loss: 1.0600 - val_accuracy: 0.7036 - lr: 0.1000\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6765 - accuracy: 0.8188 - val_loss: 0.9812 - val_accuracy: 0.7269 - lr: 0.1000\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6764 - accuracy: 0.8187 - val_loss: 0.9475 - val_accuracy: 0.7433 - lr: 0.1000\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6696 - accuracy: 0.8212 - val_loss: 1.0261 - val_accuracy: 0.7255 - lr: 0.1000\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6675 - accuracy: 0.8218 - val_loss: 1.2375 - val_accuracy: 0.6746 - lr: 0.1000\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6669 - accuracy: 0.8238 - val_loss: 0.9154 - val_accuracy: 0.7506 - lr: 0.1000\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6612 - accuracy: 0.8244 - val_loss: 0.8944 - val_accuracy: 0.7562 - lr: 0.1000\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.6579 - accuracy: 0.8258 - val_loss: 1.3982 - val_accuracy: 0.6500 - lr: 0.1000\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6583 - accuracy: 0.8262 - val_loss: 1.0165 - val_accuracy: 0.7350 - lr: 0.1000\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6582 - accuracy: 0.8261 - val_loss: 1.1111 - val_accuracy: 0.7011 - lr: 0.1000\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6468 - accuracy: 0.8314 - val_loss: 0.8506 - val_accuracy: 0.7715 - lr: 0.1000\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6493 - accuracy: 0.8301 - val_loss: 1.1824 - val_accuracy: 0.6822 - lr: 0.1000\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6478 - accuracy: 0.8307 - val_loss: 0.9124 - val_accuracy: 0.7590 - lr: 0.1000\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.6466 - accuracy: 0.8333 - val_loss: 0.9608 - val_accuracy: 0.7385 - lr: 0.1000\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6478 - accuracy: 0.8302 - val_loss: 1.1581 - val_accuracy: 0.7067 - lr: 0.1000\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6381 - accuracy: 0.8354 - val_loss: 0.9870 - val_accuracy: 0.7382 - lr: 0.1000\n",
      "Epoch 38/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6380 - accuracy: 0.8363 - val_loss: 0.9327 - val_accuracy: 0.7493 - lr: 0.1000\n",
      "Epoch 39/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6359 - accuracy: 0.8368 - val_loss: 1.5721 - val_accuracy: 0.6334 - lr: 0.1000\n",
      "Epoch 40/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6362 - accuracy: 0.8362 - val_loss: 0.9651 - val_accuracy: 0.7361 - lr: 0.1000\n",
      "Epoch 41/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6327 - accuracy: 0.8399 - val_loss: 0.9104 - val_accuracy: 0.7487 - lr: 0.1000\n",
      "Epoch 42/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6380 - accuracy: 0.8374 - val_loss: 0.8823 - val_accuracy: 0.7676 - lr: 0.1000\n",
      "Epoch 43/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.6308 - accuracy: 0.8395 - val_loss: 0.8890 - val_accuracy: 0.7686 - lr: 0.1000\n",
      "Epoch 44/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6303 - accuracy: 0.8390 - val_loss: 0.8106 - val_accuracy: 0.7875 - lr: 0.1000\n",
      "Epoch 45/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6285 - accuracy: 0.8390 - val_loss: 1.1038 - val_accuracy: 0.7168 - lr: 0.1000\n",
      "Epoch 46/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6278 - accuracy: 0.8389 - val_loss: 1.1179 - val_accuracy: 0.7189 - lr: 0.1000\n",
      "Epoch 47/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6212 - accuracy: 0.8421 - val_loss: 0.9646 - val_accuracy: 0.7508 - lr: 0.1000\n",
      "Epoch 48/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6300 - accuracy: 0.8392 - val_loss: 1.2492 - val_accuracy: 0.7058 - lr: 0.1000\n",
      "Epoch 49/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6262 - accuracy: 0.8392 - val_loss: 0.6690 - val_accuracy: 0.8301 - lr: 0.1000\n",
      "Epoch 50/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6221 - accuracy: 0.8435 - val_loss: 0.9725 - val_accuracy: 0.7520 - lr: 0.1000\n",
      "Epoch 51/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.6188 - accuracy: 0.8433 - val_loss: 0.8803 - val_accuracy: 0.7751 - lr: 0.1000\n",
      "Epoch 52/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6225 - accuracy: 0.8409 - val_loss: 0.9853 - val_accuracy: 0.7478 - lr: 0.1000\n",
      "Epoch 53/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6206 - accuracy: 0.8437 - val_loss: 1.0253 - val_accuracy: 0.7290 - lr: 0.1000\n",
      "Epoch 54/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6156 - accuracy: 0.8430 - val_loss: 0.8164 - val_accuracy: 0.7824 - lr: 0.1000\n",
      "Epoch 55/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6144 - accuracy: 0.8449 - val_loss: 0.9052 - val_accuracy: 0.7635 - lr: 0.1000\n",
      "Epoch 56/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6166 - accuracy: 0.8438 - val_loss: 1.0103 - val_accuracy: 0.7215 - lr: 0.1000\n",
      "Epoch 57/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.6139 - accuracy: 0.8454 - val_loss: 0.7684 - val_accuracy: 0.7995 - lr: 0.1000\n",
      "Epoch 58/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6149 - accuracy: 0.8451 - val_loss: 0.8831 - val_accuracy: 0.7638 - lr: 0.1000\n",
      "Epoch 59/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.6145 - accuracy: 0.8443 - val_loss: 0.8457 - val_accuracy: 0.7722 - lr: 0.1000\n",
      "Epoch 60/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6091 - accuracy: 0.8476 - val_loss: 0.8763 - val_accuracy: 0.7662 - lr: 0.1000\n",
      "Epoch 61/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6116 - accuracy: 0.8473 - val_loss: 0.8876 - val_accuracy: 0.7668 - lr: 0.1000\n",
      "Epoch 62/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6097 - accuracy: 0.8471 - val_loss: 1.0780 - val_accuracy: 0.7058 - lr: 0.1000\n",
      "Epoch 63/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.6095 - accuracy: 0.8479 - val_loss: 0.8763 - val_accuracy: 0.7813 - lr: 0.1000\n",
      "Epoch 64/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.6145 - accuracy: 0.8482 - val_loss: 0.7740 - val_accuracy: 0.8026 - lr: 0.1000\n",
      "Epoch 65/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6068 - accuracy: 0.8490 - val_loss: 0.8301 - val_accuracy: 0.7861 - lr: 0.1000\n",
      "Epoch 66/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6053 - accuracy: 0.8503 - val_loss: 0.9781 - val_accuracy: 0.7457 - lr: 0.1000\n",
      "Epoch 67/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6089 - accuracy: 0.8504 - val_loss: 1.0409 - val_accuracy: 0.7274 - lr: 0.1000\n",
      "Epoch 68/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6095 - accuracy: 0.8481 - val_loss: 0.8162 - val_accuracy: 0.7931 - lr: 0.1000\n",
      "Epoch 69/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6035 - accuracy: 0.8495 - val_loss: 1.0098 - val_accuracy: 0.7477 - lr: 0.1000\n",
      "Epoch 70/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.6055 - accuracy: 0.8485 - val_loss: 0.8190 - val_accuracy: 0.7905 - lr: 0.1000\n",
      "Epoch 71/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.5987 - accuracy: 0.8517 - val_loss: 0.9237 - val_accuracy: 0.7584 - lr: 0.1000\n",
      "Epoch 72/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.6003 - accuracy: 0.8509 - val_loss: 0.8192 - val_accuracy: 0.7823 - lr: 0.1000\n",
      "Epoch 73/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.5990 - accuracy: 0.8513 - val_loss: 0.7957 - val_accuracy: 0.7956 - lr: 0.1000\n",
      "Epoch 74/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5997 - accuracy: 0.8523 - val_loss: 0.8694 - val_accuracy: 0.7645 - lr: 0.1000\n",
      "Epoch 75/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.5945 - accuracy: 0.8533 - val_loss: 0.8637 - val_accuracy: 0.7742 - lr: 0.1000\n",
      "Epoch 76/100\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.5991 - accuracy: 0.8520 - val_loss: 1.0133 - val_accuracy: 0.7323 - lr: 0.1000\n",
      "Epoch 77/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.6027 - accuracy: 0.8513 - val_loss: 0.8025 - val_accuracy: 0.7857 - lr: 0.1000\n",
      "Epoch 78/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.5986 - accuracy: 0.8523 - val_loss: 0.8801 - val_accuracy: 0.7815 - lr: 0.1000\n",
      "Epoch 79/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5964 - accuracy: 0.8520 - val_loss: 0.9550 - val_accuracy: 0.7606 - lr: 0.1000\n",
      "Epoch 80/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.5991 - accuracy: 0.8518 - val_loss: 0.9503 - val_accuracy: 0.7495 - lr: 0.1000\n",
      "Epoch 81/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.5939 - accuracy: 0.8540 - val_loss: 1.2748 - val_accuracy: 0.6893 - lr: 0.1000\n",
      "Epoch 82/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.5990 - accuracy: 0.8523 - val_loss: 0.7735 - val_accuracy: 0.8015 - lr: 0.1000\n",
      "Epoch 83/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5205 - accuracy: 0.8817 - val_loss: 0.5574 - val_accuracy: 0.8670 - lr: 0.0100\n",
      "Epoch 84/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4826 - accuracy: 0.8930 - val_loss: 0.5496 - val_accuracy: 0.8686 - lr: 0.0100\n",
      "Epoch 85/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4682 - accuracy: 0.8971 - val_loss: 0.5457 - val_accuracy: 0.8707 - lr: 0.0100\n",
      "Epoch 86/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.4604 - accuracy: 0.8999 - val_loss: 0.5469 - val_accuracy: 0.8690 - lr: 0.0100\n",
      "Epoch 87/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.4575 - accuracy: 0.8986 - val_loss: 0.5365 - val_accuracy: 0.8728 - lr: 0.0100\n",
      "Epoch 88/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4469 - accuracy: 0.9024 - val_loss: 0.5344 - val_accuracy: 0.8715 - lr: 0.0100\n",
      "Epoch 89/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.4436 - accuracy: 0.9023 - val_loss: 0.5323 - val_accuracy: 0.8730 - lr: 0.0100\n",
      "Epoch 90/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4380 - accuracy: 0.9043 - val_loss: 0.5335 - val_accuracy: 0.8696 - lr: 0.0100\n",
      "Epoch 91/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4326 - accuracy: 0.9053 - val_loss: 0.5373 - val_accuracy: 0.8738 - lr: 0.0100\n",
      "Epoch 92/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.4287 - accuracy: 0.9046 - val_loss: 0.5346 - val_accuracy: 0.8724 - lr: 0.0100\n",
      "Epoch 93/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.4251 - accuracy: 0.9070 - val_loss: 0.5238 - val_accuracy: 0.8722 - lr: 0.0100\n",
      "Epoch 94/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.4244 - accuracy: 0.9058 - val_loss: 0.5255 - val_accuracy: 0.8714 - lr: 0.0100\n",
      "Epoch 95/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.4236 - accuracy: 0.9065 - val_loss: 0.5263 - val_accuracy: 0.8717 - lr: 0.0100\n",
      "Epoch 96/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.4141 - accuracy: 0.9085 - val_loss: 0.5242 - val_accuracy: 0.8719 - lr: 0.0100\n",
      "Epoch 97/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4119 - accuracy: 0.9093 - val_loss: 0.5206 - val_accuracy: 0.8730 - lr: 0.0100\n",
      "Epoch 98/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4069 - accuracy: 0.9104 - val_loss: 0.5168 - val_accuracy: 0.8725 - lr: 0.0100\n",
      "Epoch 99/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4066 - accuracy: 0.9087 - val_loss: 0.5190 - val_accuracy: 0.8754 - lr: 0.0100\n",
      "Epoch 100/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.4030 - accuracy: 0.9114 - val_loss: 0.5191 - val_accuracy: 0.8730 - lr: 0.0100\n",
      "Test accuracy: 87.300\n",
      "\n",
      "Running ResNet with n = 1 and p_L = 0.95...\n",
      "Epoch 1/100\n",
      "391/391 [==============================] - 14s 32ms/step - loss: 1.6756 - accuracy: 0.4034 - val_loss: 1.8383 - val_accuracy: 0.4255 - lr: 0.1000\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 1.3357 - accuracy: 0.5460 - val_loss: 1.5661 - val_accuracy: 0.5133 - lr: 0.1000\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 1.1738 - accuracy: 0.6087 - val_loss: 1.2060 - val_accuracy: 0.5989 - lr: 0.1000\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 1.0705 - accuracy: 0.6507 - val_loss: 1.2996 - val_accuracy: 0.5835 - lr: 0.1000\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 1.0333 - accuracy: 0.6663 - val_loss: 1.0597 - val_accuracy: 0.6595 - lr: 0.1000\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.9744 - accuracy: 0.6912 - val_loss: 1.0029 - val_accuracy: 0.6858 - lr: 0.1000\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.9614 - accuracy: 0.7003 - val_loss: 1.2285 - val_accuracy: 0.6453 - lr: 0.1000\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.9257 - accuracy: 0.7182 - val_loss: 1.0253 - val_accuracy: 0.6841 - lr: 0.1000\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.9009 - accuracy: 0.7265 - val_loss: 1.0199 - val_accuracy: 0.6959 - lr: 0.1000\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8958 - accuracy: 0.7311 - val_loss: 1.1764 - val_accuracy: 0.6518 - lr: 0.1000\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.8572 - accuracy: 0.7445 - val_loss: 1.2104 - val_accuracy: 0.6636 - lr: 0.1000\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.8562 - accuracy: 0.7462 - val_loss: 1.0784 - val_accuracy: 0.6867 - lr: 0.1000\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8413 - accuracy: 0.7520 - val_loss: 1.1622 - val_accuracy: 0.6928 - lr: 0.1000\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8256 - accuracy: 0.7611 - val_loss: 0.9120 - val_accuracy: 0.7406 - lr: 0.1000\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8176 - accuracy: 0.7623 - val_loss: 0.9703 - val_accuracy: 0.7187 - lr: 0.1000\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.8323 - accuracy: 0.7602 - val_loss: 1.0233 - val_accuracy: 0.7093 - lr: 0.1000\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.8130 - accuracy: 0.7680 - val_loss: 0.9610 - val_accuracy: 0.7262 - lr: 0.1000\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7734 - accuracy: 0.7821 - val_loss: 0.8918 - val_accuracy: 0.7543 - lr: 0.1000\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7820 - accuracy: 0.7808 - val_loss: 0.8996 - val_accuracy: 0.7534 - lr: 0.1000\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.8011 - accuracy: 0.7756 - val_loss: 0.8483 - val_accuracy: 0.7607 - lr: 0.1000\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7952 - accuracy: 0.7776 - val_loss: 1.0998 - val_accuracy: 0.7075 - lr: 0.1000\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.8181 - accuracy: 0.7704 - val_loss: 0.8564 - val_accuracy: 0.7609 - lr: 0.1000\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7856 - accuracy: 0.7796 - val_loss: 0.9621 - val_accuracy: 0.7354 - lr: 0.1000\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8027 - accuracy: 0.7753 - val_loss: 0.8627 - val_accuracy: 0.7571 - lr: 0.1000\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8005 - accuracy: 0.7766 - val_loss: 0.8661 - val_accuracy: 0.7590 - lr: 0.1000\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7622 - accuracy: 0.7914 - val_loss: 0.8452 - val_accuracy: 0.7625 - lr: 0.1000\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7883 - accuracy: 0.7809 - val_loss: 0.8975 - val_accuracy: 0.7527 - lr: 0.1000\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7551 - accuracy: 0.7923 - val_loss: 0.8032 - val_accuracy: 0.7779 - lr: 0.1000\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7723 - accuracy: 0.7881 - val_loss: 0.8070 - val_accuracy: 0.7724 - lr: 0.1000\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 0.7589 - accuracy: 0.7926 - val_loss: 0.8870 - val_accuracy: 0.7596 - lr: 0.1000\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7654 - accuracy: 0.7922 - val_loss: 1.0346 - val_accuracy: 0.7215 - lr: 0.1000\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7683 - accuracy: 0.7895 - val_loss: 0.8795 - val_accuracy: 0.7557 - lr: 0.1000\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7527 - accuracy: 0.7953 - val_loss: 0.8859 - val_accuracy: 0.7596 - lr: 0.1000\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7635 - accuracy: 0.7918 - val_loss: 0.8796 - val_accuracy: 0.7584 - lr: 0.1000\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7534 - accuracy: 0.7960 - val_loss: 0.9316 - val_accuracy: 0.7522 - lr: 0.1000\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7416 - accuracy: 0.7994 - val_loss: 0.8350 - val_accuracy: 0.7742 - lr: 0.1000\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7709 - accuracy: 0.7897 - val_loss: 1.0334 - val_accuracy: 0.7256 - lr: 0.1000\n",
      "Epoch 38/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7387 - accuracy: 0.8021 - val_loss: 0.8611 - val_accuracy: 0.7726 - lr: 0.1000\n",
      "Epoch 39/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7369 - accuracy: 0.8029 - val_loss: 0.8850 - val_accuracy: 0.7662 - lr: 0.1000\n",
      "Epoch 40/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7467 - accuracy: 0.7977 - val_loss: 1.1085 - val_accuracy: 0.7046 - lr: 0.1000\n",
      "Epoch 41/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7294 - accuracy: 0.8035 - val_loss: 0.9541 - val_accuracy: 0.7359 - lr: 0.1000\n",
      "Epoch 42/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7307 - accuracy: 0.8061 - val_loss: 0.8520 - val_accuracy: 0.7641 - lr: 0.1000\n",
      "Epoch 43/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.7413 - accuracy: 0.8004 - val_loss: 0.8775 - val_accuracy: 0.7638 - lr: 0.1000\n",
      "Epoch 44/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7508 - accuracy: 0.7973 - val_loss: 0.8523 - val_accuracy: 0.7679 - lr: 0.1000\n",
      "Epoch 45/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7361 - accuracy: 0.8018 - val_loss: 0.8579 - val_accuracy: 0.7701 - lr: 0.1000\n",
      "Epoch 46/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7403 - accuracy: 0.8014 - val_loss: 0.7412 - val_accuracy: 0.8032 - lr: 0.1000\n",
      "Epoch 47/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7524 - accuracy: 0.7977 - val_loss: 0.8935 - val_accuracy: 0.7515 - lr: 0.1000\n",
      "Epoch 48/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7181 - accuracy: 0.8099 - val_loss: 0.8172 - val_accuracy: 0.7809 - lr: 0.1000\n",
      "Epoch 49/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7283 - accuracy: 0.8063 - val_loss: 0.9675 - val_accuracy: 0.7258 - lr: 0.1000\n",
      "Epoch 50/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7613 - accuracy: 0.7940 - val_loss: 1.1362 - val_accuracy: 0.7164 - lr: 0.1000\n",
      "Epoch 51/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7218 - accuracy: 0.8085 - val_loss: 1.2465 - val_accuracy: 0.6476 - lr: 0.1000\n",
      "Epoch 52/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7465 - accuracy: 0.8005 - val_loss: 0.8819 - val_accuracy: 0.7555 - lr: 0.1000\n",
      "Epoch 53/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7351 - accuracy: 0.8035 - val_loss: 1.6582 - val_accuracy: 0.5942 - lr: 0.1000\n",
      "Epoch 54/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7381 - accuracy: 0.8004 - val_loss: 0.9678 - val_accuracy: 0.7328 - lr: 0.1000\n",
      "Epoch 55/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7134 - accuracy: 0.8124 - val_loss: 0.8821 - val_accuracy: 0.7603 - lr: 0.1000\n",
      "Epoch 56/100\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 0.7251 - accuracy: 0.8069 - val_loss: 0.9692 - val_accuracy: 0.7412 - lr: 0.1000\n",
      "Epoch 57/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7257 - accuracy: 0.8072 - val_loss: 0.8360 - val_accuracy: 0.7775 - lr: 0.1000\n",
      "Epoch 58/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7191 - accuracy: 0.8108 - val_loss: 0.9774 - val_accuracy: 0.7375 - lr: 0.1000\n",
      "Epoch 59/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7328 - accuracy: 0.8050 - val_loss: 0.8040 - val_accuracy: 0.7818 - lr: 0.1000\n",
      "Epoch 60/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7203 - accuracy: 0.8099 - val_loss: 0.8472 - val_accuracy: 0.7800 - lr: 0.1000\n",
      "Epoch 61/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7291 - accuracy: 0.8054 - val_loss: 0.8333 - val_accuracy: 0.7775 - lr: 0.1000\n",
      "Epoch 62/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7337 - accuracy: 0.8054 - val_loss: 1.0450 - val_accuracy: 0.7166 - lr: 0.1000\n",
      "Epoch 63/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.7041 - accuracy: 0.8156 - val_loss: 1.0279 - val_accuracy: 0.7237 - lr: 0.1000\n",
      "Epoch 64/100\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 0.7152 - accuracy: 0.8101 - val_loss: 0.8799 - val_accuracy: 0.7685 - lr: 0.1000\n",
      "Epoch 65/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7083 - accuracy: 0.8129 - val_loss: 0.9307 - val_accuracy: 0.7589 - lr: 0.1000\n",
      "Epoch 66/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7281 - accuracy: 0.8056 - val_loss: 1.0796 - val_accuracy: 0.7139 - lr: 0.1000\n",
      "Epoch 67/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7141 - accuracy: 0.8117 - val_loss: 0.8981 - val_accuracy: 0.7514 - lr: 0.1000\n",
      "Epoch 68/100\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.7379 - accuracy: 0.8049 - val_loss: 0.9725 - val_accuracy: 0.7512 - lr: 0.1000\n",
      "Epoch 69/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7316 - accuracy: 0.8045 - val_loss: 0.7791 - val_accuracy: 0.7932 - lr: 0.1000\n",
      "Epoch 70/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7344 - accuracy: 0.8053 - val_loss: 0.9154 - val_accuracy: 0.7506 - lr: 0.1000\n",
      "Epoch 71/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7143 - accuracy: 0.8116 - val_loss: 0.7996 - val_accuracy: 0.7884 - lr: 0.1000\n",
      "Epoch 72/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7328 - accuracy: 0.8040 - val_loss: 0.8019 - val_accuracy: 0.7901 - lr: 0.1000\n",
      "Epoch 73/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.7220 - accuracy: 0.8075 - val_loss: 1.0317 - val_accuracy: 0.7395 - lr: 0.1000\n",
      "Epoch 74/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7136 - accuracy: 0.8130 - val_loss: 0.9858 - val_accuracy: 0.7404 - lr: 0.1000\n",
      "Epoch 75/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7171 - accuracy: 0.8105 - val_loss: 0.8559 - val_accuracy: 0.7691 - lr: 0.1000\n",
      "Epoch 76/100\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 0.7343 - accuracy: 0.8029 - val_loss: 1.0680 - val_accuracy: 0.7238 - lr: 0.1000\n",
      "Epoch 77/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7216 - accuracy: 0.8099 - val_loss: 0.9922 - val_accuracy: 0.7417 - lr: 0.1000\n",
      "Epoch 78/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7251 - accuracy: 0.8090 - val_loss: 0.8968 - val_accuracy: 0.7573 - lr: 0.1000\n",
      "Epoch 79/100\n",
      "391/391 [==============================] - 12s 29ms/step - loss: 0.7153 - accuracy: 0.8097 - val_loss: 0.9105 - val_accuracy: 0.7699 - lr: 0.1000\n",
      "Epoch 80/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7138 - accuracy: 0.8122 - val_loss: 0.9690 - val_accuracy: 0.7453 - lr: 0.1000\n",
      "Epoch 81/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.7174 - accuracy: 0.8106 - val_loss: 0.9629 - val_accuracy: 0.7388 - lr: 0.1000\n",
      "Epoch 82/100\n",
      "391/391 [==============================] - 11s 28ms/step - loss: 0.7149 - accuracy: 0.8119 - val_loss: 0.9376 - val_accuracy: 0.7522 - lr: 0.1000\n",
      "Epoch 83/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.6391 - accuracy: 0.8408 - val_loss: 0.6061 - val_accuracy: 0.8531 - lr: 0.0100\n",
      "Epoch 84/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.6051 - accuracy: 0.8519 - val_loss: 0.5883 - val_accuracy: 0.8570 - lr: 0.0100\n",
      "Epoch 85/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5892 - accuracy: 0.8558 - val_loss: 0.5818 - val_accuracy: 0.8616 - lr: 0.0100\n",
      "Epoch 86/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5737 - accuracy: 0.8601 - val_loss: 0.5719 - val_accuracy: 0.8654 - lr: 0.0100\n",
      "Epoch 87/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5765 - accuracy: 0.8596 - val_loss: 0.5701 - val_accuracy: 0.8664 - lr: 0.0100\n",
      "Epoch 88/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5543 - accuracy: 0.8657 - val_loss: 0.5637 - val_accuracy: 0.8676 - lr: 0.0100\n",
      "Epoch 89/100\n",
      "391/391 [==============================] - 12s 32ms/step - loss: 0.5376 - accuracy: 0.8708 - val_loss: 0.5702 - val_accuracy: 0.8659 - lr: 0.0100\n",
      "Epoch 90/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5786 - accuracy: 0.8548 - val_loss: 0.5691 - val_accuracy: 0.8634 - lr: 0.0100\n",
      "Epoch 91/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5461 - accuracy: 0.8672 - val_loss: 0.5648 - val_accuracy: 0.8660 - lr: 0.0100\n",
      "Epoch 92/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5295 - accuracy: 0.8703 - val_loss: 0.5597 - val_accuracy: 0.8660 - lr: 0.0100\n",
      "Epoch 93/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5373 - accuracy: 0.8685 - val_loss: 0.5558 - val_accuracy: 0.8685 - lr: 0.0100\n",
      "Epoch 94/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5205 - accuracy: 0.8732 - val_loss: 0.5439 - val_accuracy: 0.8693 - lr: 0.0100\n",
      "Epoch 95/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5181 - accuracy: 0.8753 - val_loss: 0.5476 - val_accuracy: 0.8683 - lr: 0.0100\n",
      "Epoch 96/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5151 - accuracy: 0.8741 - val_loss: 0.5572 - val_accuracy: 0.8641 - lr: 0.0100\n",
      "Epoch 97/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5280 - accuracy: 0.8687 - val_loss: 0.5510 - val_accuracy: 0.8654 - lr: 0.0100\n",
      "Epoch 98/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.5076 - accuracy: 0.8760 - val_loss: 0.5487 - val_accuracy: 0.8644 - lr: 0.0100\n",
      "Epoch 99/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.5596 - accuracy: 0.8576 - val_loss: 0.5367 - val_accuracy: 0.8677 - lr: 0.0100\n",
      "Epoch 100/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.4911 - accuracy: 0.8829 - val_loss: 0.5523 - val_accuracy: 0.8617 - lr: 0.0100\n",
      "Test accuracy: 86.170\n",
      "\n",
      "Running ResNet with n = 1 and p_L = 0.9...\n",
      "Epoch 1/100\n",
      "391/391 [==============================] - 13s 29ms/step - loss: 1.6824 - accuracy: 0.3966 - val_loss: 2.7912 - val_accuracy: 0.2826 - lr: 0.1000\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 1.3816 - accuracy: 0.5235 - val_loss: 1.3509 - val_accuracy: 0.5300 - lr: 0.1000\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 1.2555 - accuracy: 0.5789 - val_loss: 1.3646 - val_accuracy: 0.5357 - lr: 0.1000\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 1.1660 - accuracy: 0.6137 - val_loss: 1.1819 - val_accuracy: 0.6226 - lr: 0.1000\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 1.1146 - accuracy: 0.6360 - val_loss: 1.2672 - val_accuracy: 0.5724 - lr: 0.1000\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 1.0619 - accuracy: 0.6578 - val_loss: 1.0490 - val_accuracy: 0.6655 - lr: 0.1000\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 1.0473 - accuracy: 0.6688 - val_loss: 1.0916 - val_accuracy: 0.6570 - lr: 0.1000\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 1.0176 - accuracy: 0.6838 - val_loss: 1.5045 - val_accuracy: 0.5632 - lr: 0.1000\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.9670 - accuracy: 0.7029 - val_loss: 0.9926 - val_accuracy: 0.7053 - lr: 0.1000\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.9510 - accuracy: 0.7099 - val_loss: 0.8727 - val_accuracy: 0.7427 - lr: 0.1000\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.9201 - accuracy: 0.7205 - val_loss: 1.0814 - val_accuracy: 0.6587 - lr: 0.1000\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.9258 - accuracy: 0.7233 - val_loss: 0.9854 - val_accuracy: 0.6902 - lr: 0.1000\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8997 - accuracy: 0.7312 - val_loss: 0.9164 - val_accuracy: 0.7326 - lr: 0.1000\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8877 - accuracy: 0.7357 - val_loss: 1.0376 - val_accuracy: 0.6874 - lr: 0.1000\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 13s 32ms/step - loss: 0.8965 - accuracy: 0.7334 - val_loss: 1.0751 - val_accuracy: 0.6982 - lr: 0.1000\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8685 - accuracy: 0.7457 - val_loss: 0.8534 - val_accuracy: 0.7580 - lr: 0.1000\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8882 - accuracy: 0.7402 - val_loss: 0.9525 - val_accuracy: 0.7200 - lr: 0.1000\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8422 - accuracy: 0.7577 - val_loss: 0.9015 - val_accuracy: 0.7359 - lr: 0.1000\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8528 - accuracy: 0.7541 - val_loss: 1.0201 - val_accuracy: 0.6997 - lr: 0.1000\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.8593 - accuracy: 0.7521 - val_loss: 0.9717 - val_accuracy: 0.7351 - lr: 0.1000\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8581 - accuracy: 0.7517 - val_loss: 1.0401 - val_accuracy: 0.7143 - lr: 0.1000\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.8407 - accuracy: 0.7608 - val_loss: 1.1741 - val_accuracy: 0.6801 - lr: 0.1000\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.8507 - accuracy: 0.7582 - val_loss: 1.0253 - val_accuracy: 0.7081 - lr: 0.1000\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8538 - accuracy: 0.7585 - val_loss: 1.0105 - val_accuracy: 0.7251 - lr: 0.1000\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.8391 - accuracy: 0.7625 - val_loss: 0.9384 - val_accuracy: 0.7269 - lr: 0.1000\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8947 - accuracy: 0.7427 - val_loss: 0.8833 - val_accuracy: 0.7572 - lr: 0.1000\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8241 - accuracy: 0.7689 - val_loss: 0.8495 - val_accuracy: 0.7580 - lr: 0.1000\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.8504 - accuracy: 0.7574 - val_loss: 1.3337 - val_accuracy: 0.6412 - lr: 0.1000\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8162 - accuracy: 0.7717 - val_loss: 0.9395 - val_accuracy: 0.7357 - lr: 0.1000\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - 12s 31ms/step - loss: 0.8232 - accuracy: 0.7694 - val_loss: 0.8811 - val_accuracy: 0.7531 - lr: 0.1000\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8323 - accuracy: 0.7668 - val_loss: 0.9917 - val_accuracy: 0.7315 - lr: 0.1000\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8327 - accuracy: 0.7662 - val_loss: 0.9692 - val_accuracy: 0.7301 - lr: 0.1000\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8228 - accuracy: 0.7699 - val_loss: 0.8672 - val_accuracy: 0.7650 - lr: 0.1000\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - 11s 29ms/step - loss: 0.8118 - accuracy: 0.7753 - val_loss: 0.9931 - val_accuracy: 0.7231 - lr: 0.1000\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - 12s 30ms/step - loss: 0.8003 - accuracy: 0.7781 - val_loss: 0.9231 - val_accuracy: 0.7447 - lr: 0.1000\n",
      "Epoch 36/100\n",
      "331/391 [========================>.....] - ETA: 1s - loss: 0.8214 - accuracy: 0.7691"
     ]
    }
   ],
   "source": [
    "# Run CIFAR-10 experiments.\n",
    "n_out = 10\n",
    "ns = [1, 3, 5, 7, 9]\n",
    "p_Ls = [1, 0.95, 0.9, 0.85, 0.8]\n",
    "\n",
    "for n in ns:\n",
    "    for p_L in p_Ls:\n",
    "        print(f\"Running ResNet with n = {n} and p_L = {p_L}...\")\n",
    "        model = ResNet(n, n_out, p_L)\n",
    "        callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        t_start = time.time()\n",
    "        history = model.fit(train_ds, epochs=epochs, validation_data=(X_test, y_test), callbacks=[callback], verbose=1)\n",
    "        t = time.time() - t_start\n",
    "        save_result(n, p_L, history, t, path)\n",
    "        tf.keras.backend.clear_session()\n",
    "        _ = gc.collect()\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-100 data.\n",
    "(X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# Initialize normalization layer.\n",
    "norm_layer = Normalization(axis=(1, 2, 3), mean=X_train.mean(axis=0), variance=1)\n",
    "\n",
    "# Convert from integers to floats.\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Apply normalization.\n",
    "X_train = norm_layer(X_train)\n",
    "X_test = norm_layer(X_test)\n",
    "\n",
    "# Data augmentation.\n",
    "# Pad X_train.\n",
    "X_train = pad(X_train, [[0, 0], [4, 4], [4, 4], [0, 0]])\n",
    "\n",
    "# Augmentation function.\n",
    "def augment(image_label, seed):\n",
    "    image, label = image_label\n",
    "    \n",
    "    # Make a new seed.\n",
    "    new_seed = stateless_split(seed, num=1)[0, :]\n",
    "    \n",
    "    # Randomly flip and crop.\n",
    "    image = stateless_random_flip_left_right(image, seed=seed)\n",
    "    image = stateless_random_crop(image, size=[32, 32, 3], seed=new_seed)\n",
    "    return image, label\n",
    "\n",
    "rng = tf.random.Generator.from_seed(123, alg='philox')\n",
    "# Wrapper function.\n",
    "def f(x, y):\n",
    "    seed = rng.make_seeds(2)[0]\n",
    "    image, label = augment((x, y), seed)\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_ds = train_ds.shuffle(1280, reshuffle_each_iteration=True).map(f, num_parallel_calls=AUTOTUNE).batch(128).prefetch(AUTOTUNE)\n",
    "\n",
    "path = 'cifar100/'\n",
    "with open(path + 'results.csv', 'w') as results:\n",
    "    writer = csv.writer(results)\n",
    "    writer.writerow(['n', 'p_L', 'acc', 'time (s)', 'time (min)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy model to get accurate time measurements.\n",
    "n = 2\n",
    "n_out = 100\n",
    "p_L = 0.9\n",
    "model = ResNet(n, n_out, p_L)\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "history = model.fit(train_ds, epochs=1, validation_data=(X_test, y_test), callbacks=[callback], verbose=0)\n",
    "tf.keras.backend.clear_session()\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hsQPS9HeW0HK",
    "outputId": "540105f8-5acd-47ae-ffcd-3f284d1782d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run CIFAR-100 experiments.\n",
    "n_out = 100\n",
    "ns = [1, 3, 5, 7, 9]\n",
    "p_Ls = [1, 0.95, 0.9, 0.85, 0.8]\n",
    "\n",
    "for n in ns:\n",
    "    for p_L in p_Ls:\n",
    "        print(f\"Running ResNet with n = {n} and p_L = {p_L}...\")\n",
    "        model = ResNet(n, n_out, p_L)\n",
    "        callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        t_start = time.time()\n",
    "        history = model.fit(train_ds, epochs=epochs, validation_data=(X_test, y_test), callbacks=[callback], verbose=1)\n",
    "        t = time.time() - t_start\n",
    "        save_result(n, p_L, history, t, path)\n",
    "        tf.keras.backend.clear_session()\n",
    "        _ = gc.collect()\n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
